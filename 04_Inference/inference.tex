
%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\graphicspath{{Fig/}}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{float}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{amsmath}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Calculus]{Mathematics/Statistics Bootcamp} % The short title appears at the bottom of every slide, the full title is only on the title page

\subtitle{Part IV: Basics of Statistical Inference}

\author{Fan Bu\inst{1} \and Kyle Burris\inst{1}}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Duke University] % (optional, but mostly needed)
{
  \inst{1}%
  Department of Statistical Science\\
  Duke University
  }
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date{Graduate Orientation, August 2018}

\begin{document}
\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------
\section{Limiting Theorems}
\begin{frame}
\frametitle{The Law of Large Numbers (LLN)}
Suppose $\{X_1,X_2,\ldots\}$ is a sequence of independently and identically distributed (i.i.d.) random variables with $E[X_i] = \mu$. Let $\bar{X}_n = \frac{\sum_{i=1}^n X_i}{n}$ be the sample average. Then:
\begin{itemize}
\item The \textbf{Weak Law}: $\bar{X}_n \xrightarrow[]{p} \mu$ when $n \rightarrow \infty$, that is, for any $\epsilon > 0$,
$$
\lim_{n \rightarrow \infty}P(\vert \bar{X}_n -\mu\vert > \epsilon) = 0.
$$
\item The \textbf{Strong Law}: $\bar{X}_n \xrightarrow[]{a.s.} \mu$ when $n \rightarrow \infty$, that is, 
$$
P\left( \lim_{n \rightarrow \infty}\bar{X}_n =\mu \right) = 1.
$$
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{The Central Limit Theorem (CLT)}
Suppose $\{X_1,X_2,\ldots\}$ is a sequence of independently and identically distributed (i.i.d.) random variables with $E[X_i] = \mu$ and $Var[X_i] = \sigma^2 < \infty$. Let $\bar{X}_n = \frac{\sum_{i=1}^n X_i}{n}$ be the sample average, then as $n \rightarrow \infty$, the random variable $\sqrt{n} (\bar{X}_n - \mu)$ converges in distribution to $N(0,\sigma^2)$:
$$
\sqrt{n} (\bar{X}_n - \mu) \xrightarrow[]{d} N(0,\sigma^2).
$$
\end{frame}

\begin{frame}
\frametitle{Mini-exercises}
\begin{enumerate}
\item Rewrite the CLT in terms of the sample sum, $S_n = \sum_{i=1}^n X_i$.
\vspace*{1in}
\item Let $\{X_1,X_2,\ldots,X_n\}$ be a sequence of $n$ independent results from tossing the same fair coin where $X_i = 1$ when the head faces up and $X_i = 0$ otherwise. Let $\bar{X}_n = \frac{\sum_{i=1}^n X_i}{n}$. If $n=100$, estimate the value of 
$$
P(0.4 < \bar{X}_n < 0.6).
$$
% answer: approximately 0.95.
\vspace*{0.7in}
\end{enumerate}
\end{frame}

%------------------------------------------------
\section{Data Reduction}
\begin{frame}
\frametitle{Sufficiency}
\begin{itemize}
\item \textbf{Definition}: A statistic $T(X)$ is a \textbf{sufficient statistic for $\theta$} if the conditional  distribution of the sample $X$ given the value of $T(X)$ does not depend on $\theta$.
\item \textbf{Sufficiency Principle}: If $T(X)$ is a sufficient statistic for $\theta$, then any inference about $\theta$ should depend on the sample $X$ only through the value $T(X)$. That is, if $x$ and $y$ are two sample points such that $T(X) = T(Y)$, then the inference about $\theta$ should be the same whether $X=x$ or $Y=y$ is observed.
\item \textbf{Factorization Theorem}: Let $f(x|\theta)$ denote the pdf or pmf of a sample $X$. A statistic $T(X)$ is a sufficient statistic for $\theta$ if and only if there exist functions $g(t|\theta)$ and $h(x)$ such that, for all sample points $x$ and all parameter points $\theta$,
$$
f(x|\theta) = g(T(x)|\theta)h(x).
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sufficiency: An Exercise}
Let $X_1,X_2,\ldots,X_n$ be i.i.d. $N(\mu,\sigma^2)$ where $\sigma^2$ is known. Show that the sample mean, $T(X) = \bar{X} = (X_1+X_2+\cdots+X_n)/n$, is a sufficient statistic for $\mu$.
\\~\\
Note: the joint pdf of the sample $\mathbf{X}$ is
$$
\begin{aligned}
f(\mathbf{x}|\mu) &= \prod_{i=1}^n (2\pi \sigma^2)^{-1/2} \exp(-(x_i - \mu)^2/(2\sigma^2))\\
&=(2\pi \sigma^2)^{-1/2} \exp(-\sum_{i=1}^n(x_i - \mu)^2/(2\sigma^2)).
\end{aligned}
$$
~\\
- If $\sigma^2$ is also unknown, what is the sufficient statistic of $\theta=(\mu,\sigma^2)$ ?\\
% Answer: $T_1(X) = \bar{X}$ and $T_2(X) = S^2 = \sum_{i=1}^n(X_i - \bar{X})^2/(n-1)$
\end{frame}

% \begin{frame}
% \frametitle{Completeness}
% \begin{itemize}
% \item \textbf{Ancillary Statistic}: A statistic $S(X)$ whose distribution does not depend on the parameter $\theta$ is called an \textbf{ancillary statistic}.
% \item \textbf{Complete Statistic}: Let $f(t|\theta)$ be a family of pdfs or pmfs for a statistic $T(X)$. The family of distributions is called \textbf{complete} (equivalently, $T(X)$ is called a complete statistic) if for any function $g$,
% $$
% E_{\theta} g(T) = 0 \text{ for all } \theta \quad\Rightarrow\quad P_{\theta}(g(T)=0)=1 \text{ for all } \theta.
% $$
% \end{itemize}
% \end{frame}

\begin{frame}
%\frametitle{Complete and Sufficient Statistics for the Exponential Family}
\frametitle{Sufficient Statistics for the Exponential Family}
Let $X_1,X_2,\ldots,X_n$ be i.i.d observations from an exponential family with pdf or pmf of the form
$$
f(x|\theta) = h(x)c(\theta)\exp\left(\sum_{j=1}^k w(\theta_j)t_j(x)\right),
$$
where $\theta = (\theta_1,\theta_2,\ldots,\theta_k)$. Then the statistic
$$
T(X) = \left(\sum_{i=1}^n t_1(X_i),\sum_{i=1}^n t_2(X_i),\ldots,\sum_{i=1}^n t_k(X_i)\right)
$$
%is a complete and sufficient statistic (c.s.s.) as long as the parameter space $\Theta$ contains an open set in $\mathbb{R}^k$.
is a sufficient statistic.
\end{frame}

\begin{frame}
%\frametitle{Completeness and Sufficiency: Exercises}
\frametitle{Sufficiency: Exercises}
\begin{enumerate}
\item Suppose that $X_1,X_2,\ldots,X_n$ are i.i.d observations from a Poisson distribution with parameter $\lambda$. %Find a c.s.s. for $\lambda$.
Find a sufficient statistic for $\lambda$.
\vspace*{0.7in}
\item Suppose that $X_1,X_2,\ldots,X_n$ are i.i.d observations from a Gamma distribution with parameters $\theta = (\alpha,\beta)$ (use the shape-rate parametrization). %Find a c.s.s. for $\theta$.
Find a sufficient statistic for $\theta$.
\end{enumerate}
\end{frame}

%------------------------------------------------
\section{Point Estimation}
\begin{frame}
\frametitle{Point Estimation}
\begin{itemize}
\item A \textbf{point estimator} is any function of the sample.
\item \textbf{Estimator} vs. \textbf{Estimate}: The former is a function, while the latter is the realized value of the function (a number) that is obtained when a sample is actually taken.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum Likelihood Estimators}
\begin{itemize}
\item If $X_1,\ldots,X_n$ are an i.i.d. sample from a population with pdf or pmf $f(\mathbf{x}|\theta_1,\ldots,\theta_k)$, the \textbf{likelihood function} is
$$
L(\mathbf{\theta}|\mathbf{x}) = L(\theta_1,\ldots,\theta_k|x_1,\ldots,x_n) = \prod_{i=1}^n f(x_i|\theta_1,\ldots,\theta_k).
$$
\item For each sample point $\mathbf{x}$, let $\hat{\theta}(\mathbf{x})$ be a parameter value at which $L(\mathbf{\theta}|\mathbf{x})$ attains its maximum as a function of $\mathbf{\theta}$, with $\mathbf{x}$ held fixed. A \textbf{maximum likelihood estimator (MLE)} of the parameter $\mathbf{\theta}$ based on a sample $\mathbf{X}$ is $\hat{\theta}(\mathbf{X})$.
\item If the likelihood function is differentiable (in $\theta_i$), \textbf{possible candidates} for the MLE are the values of $(\theta_1,\ldots,\theta_k)$ that satisfy
$$
\frac{\partial}{\partial \theta_i} L(\mathbf{\theta}|\mathbf{x})=0, \quad i=1,\ldots,k.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MLE: Normal Example}
Let $X_1,\ldots,X_n$ be i.i.d. $N(\theta,1)$, and let $L(\theta|\mathbf{x})$ denote the likelihood function. Since maximizing
$$
L(\theta|\mathbf{x}) = \frac{1}{(2\pi)^{n/2}}e^{(-1/2)\sum_{i=1}^n(x_i-\theta)^2},
$$
is equivalent to maximizing $\ln L(\theta|\mathbf{x})$, which reduces to maxizing 
$$
h(\theta)=(-1/2)\sum_{i=1}^n(x_i-\theta)^2,
$$
a quadratic function of $\theta$.
\\~\\
Since $\hat{\theta}=\bar{x}=(\sum_{i=1}^n x_i)/n$ is the global maxima of $h(\theta)$, it is also the global maximum of $L(\theta|\mathbf{x})$. Therefore $\hat{\theta}$ is the MLE.
\end{frame}

\begin{frame}
\frametitle{MLE: Exercises}
\begin{enumerate}
\item Let $X_1,\ldots,X_n$ be i.i.d. samples from the uniform distribution $U(0,\theta)$, $\theta > 0$. Find the MLE of $\theta$.
% answer: $\max{X_i}$
\vspace*{1in}
\item Let $X_1,\ldots,X_n$ be i.i.d. $\text{Bernoulli}(p)$. Find the MLE of $p$.
% answer: the sample mean $\sum_{i=1}^n X_i/n$.
\vspace*{1in}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{The Invariance Property of MLEs}
\begin{theorem}
If $\hat{\theta}$ is the MLE of $\theta$, then for any function $\tau(\theta)$ of $\theta$, the MLE of $\tau(\theta)$ is $\tau(\hat{\theta})$.
\end{theorem}

\vspace*{0.7in}
A mini-exercise: following ex.2 from above, what is the MLE of $\sqrt{p(1-p)}$?
\end{frame}

\begin{frame}
\frametitle{Mean Squared Error (MSE) and Bias}
\begin{itemize}
\item The \textbf{mean squared error (MSE)} of an estimator $W$ of a parameter $\theta$ is defined by $E_{\theta}(W-\theta)^2$.
\item The \textbf{bias} of a point estimator $W$ of a parameter $\theta$ is $\text{Bias}_{\theta}W=E_{\theta}W-\theta$, and an estimator is called \textbf{unbiased} if $E_{\theta}W=\theta$ for all $\theta$.
\item Relationship between MSE and bias:
$$
E_{\theta}(W-\theta)^2 = \text{Var}_{\theta}W + (\text{Bias}_{\theta}W)^2.
$$
\item If $W$ is an unbiased estimator of $\theta$,
$$
E_{\theta}(W-\theta)^2 = \text{Var}_{\theta}W.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MSE and Bias: An Exercise}
Let $X_1,\ldots,X_n$ be i.i.d. $N(\mu,\sigma^2)$, $\bar{X}=(\sum_{i=1}^n X_i)/n$ be the sample mean, and $S^2=\sum_{i=1}^n (X_i-\bar{X})^2/(n-1)$ be the sample variance. Verify that $\bar{X}$ and $S^2$ are unbiased estimators for $\mu$ and $\sigma^2$, respectively, and compute their MSEs.\\
(Note: if $Y \sim \chi^2$, then $\text{Var}(Y)=2$.)
%\\ answer: $E(\bar{X}-\mu)^2=\frac{\sigma^2}{n}$, $E(S^2-\sigma^2)^2=\frac{2\sigma^4}{n-1}$.
\vspace*{0.6in}

If we adopt the MLE estimator $\hat{\sigma}^2$ for $\sigma^2$ instead, where $\hat{\sigma}^2=\sum_{i=1}^n (X_i-\bar{X})^2/n$. What is the MSE of $\hat{\sigma}^2$ ?
%\\ answer: $\left(\frac{2n-1}{n^2}\right)\sigma^4$
\vspace*{0.6in}
\end{frame}

% \begin{frame}
% \frametitle{Uniform Minimum Variance Unbiased Estimators}
% \begin{itemize}
% \item If an estimator $W^*$ of $\tau(\theta)$ satisfies $E_{\theta}W^* = \tau(\theta)$ for all $\theta$ and, for any other estimator $W$ with $E_{\theta}W = \tau(\theta)$, we have $\text{Var}_{\theta}W* \leq \text{Var}_{\theta}W$ for all $\theta$, then \textbf{$W^*$} is called a \textbf{uniform minimum variance unbiased estimator (UMVUE) of $\tau(\theta)$}.
% \item Let $T$ be a complete and sufficient statistic for a parameter $\theta$, and let $\phi(T)$ be an estimator based only on $T$ and unbiased for $\tau(\theta)$. Then $\phi(T)$ is the UMVUE of $\tau(\theta)$.
% \end{itemize}
% \end{frame}

\begin{frame}
\frametitle{Review Exercises: Morning Session}
\begin{columns}[t]
\column{.47\textwidth}
1. Let $X_1,X_2,\ldots,X_n$ be i.i.d. $\text{Poisson}(\lambda)$. Let $Y_n = (\sum_{i=1}^n X_i - \lambda)/\sqrt{n}$. When $n$ is large, $Y_n$ is approximately a normal variable with mean $\mu$ and variance $\sigma^2$. What are $\mu$ and $\sigma^2$ ?

\column{.47\textwidth}
2. Let $X_1,X_2,\ldots,X_n$ be i.i.d. $\text{Exp}(\beta)$ with pdf $f(x)=\beta e^{-\beta x} (x > 0)$. Show that $\bar{X} = \sum_{i=1}^n X_i/n$ is a sufficient statistic for $\beta$, and find the MLE of $\beta$.
\\~\\
3. Let $X$ be a sample from $N(0,\sigma^2)$. Find an unbiased estimator of $\sigma^2$.

\end{columns}
\end{frame}

%------------------------------------------------

\section{Hypothesis Testing}

\begin{frame}
\frametitle{Intro to Hypothesis Testing}
\href{https://youtu.be/VK-rnA3-41c}{\textbf{Video tutorial}, by \textit{mathtutordvd}}
\end{frame}

%\begin{frame}
%\frametitle{Challenge Exercises: Morning Session}
%\end{frame}

\begin{frame}
\frametitle{Hypothesis Testing: Likelihood Ratio Tests}
The \textbf{likelihood ratio test statistic} for testing $H_0: \theta \in \Theta_0$ versus $H_1: \theta \in \Theta_0^{c}$ is
$$
\lambda(\mathbf{x}) = \frac{\sup_{\Theta_0} L(\mathbf{\theta}|\mathbf{x})}{\sup_{\Theta} L(\mathbf{\theta}|\mathbf{x})}.
$$
A \textbf{likelihood ratio test (LRT)} is any test that has a rejection region of the form $\{\mathbf{x}: \lambda(\mathbf{x})\leq c\}$, where $c$ is a number satisfying $0\leq c\leq 1$.
\\~\\
Suppose $\hat{\theta}$ is an MLE of $\theta$ (obtained by the unrestricted maximization of $L(\mathbf{\theta}|\mathbf{x})$), and $\hat{\theta}_0$ is the MLE of $\theta$ assuming the parameter space is $\Theta_0$ ((obtained by maximizing $L(\mathbf{\theta}|\mathbf{x})$) on $\Theta_0$). Then the LRT statistic is
$$
\lambda(\mathbf{x}) = \frac{L(\mathbf{\hat\theta}_0|\mathbf{x})}{L(\mathbf{\hat\theta}|\mathbf{x})}.
$$
\end{frame}

\begin{frame}
\frametitle{LRT and Sufficiency}
\begin{theorem}
If $T(\mathbf{X})$ is a sufficient statistic for $\theta$ and $\lambda^*(t)$ and $\lambda(\mathbf{x})$ are the LRT statistics based on $T$ and $\mathbf{X}$, respectively, then $\lambda^*(T(\mathbf{x}))=\lambda(\mathbf{x})$ for every $\mathbf{x}$ in the sample space.
\end{theorem}
\end{frame}

\begin{frame}
\frametitle{An Exercise: Normal LRT}
Let $X_1,\ldots,N_n$ be i.i.d. $N(\theta,1)$. Consider the test $H_0: \theta=\theta_0$ versus $H_1: \theta \neq \theta_0$. Find the LRT statistic and derive the form of the rejection region.
\\~\\
%Answer: $\lambda(\mathbf{x})=\exp[-n(\bar{x}-\theta_0)^2/2]$, and the rejection region $\{\mathbf{x}: \lambda(\mathbf{x})\leq c\}$ can be written as $\{\mathbf{x}:\vert \bar{x}-\theta_0\vert \geq \sqrt{-2(\log c)/n}\}$.
\end{frame}

\begin{frame}
\frametitle{Test Errors and Power Function}
\begin{itemize}
\item Type I Error and Type II Error:
\begin{table}
\centering
\begin{tabular}{cc|c|c|}
\toprule
 &  & \multicolumn{2}{|c|}{Decision}\\
 &  & Accept $H_0$ & Reject $H_0$ \\
\hline 
\multirow{2}{*}{Truth} & $H_0$ & Correct decision & Type I Error\\
\cline{3-4}
 & $H_1$ & Type II Error & Correct decision\\
\bottomrule
\end{tabular}
\end{table}
\item Suppose $R$ denotes the rejection region for a test, then the probability of a Type I Error is $P_{\theta}(\mathbf{X}\in R|H_0)$, and the probability of a Type II Error is $P_{\theta}(\mathbf{X}\in R^c|H_1)=1-P_{\theta}(\mathbf{X}\in R|H_1)$.
\item The \textbf{power function} of a hypothesis test with rejection region $R$ is the function of $\theta$ defined by $\beta(\theta)= P_{\theta}(\mathbf{X}\in R)$.
\item For $0\leq \alpha \leq 1$, a test with power function $\beta(\theta)$ is a \textbf{level $\alpha$ test} if $\sup_{\theta \in \Theta_0} \beta(\theta) \leq \alpha$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{An Exercise: Binomial}
Let $X \sim \text{Binomial}(5,\theta)$. Consider the test $H_0: \theta \leq 1/2$ versus $H_1: \theta > 1/2$. If we adopt the test that rejects $H_0$ only if $X=5$ is observed. What is the power function of this test? How small is the probability of Type I Error? For what values of $\theta$ is the probability of Type II Error less than $\frac{1}{2}$?
\\~\\
%Answer: $\beta(\theta)=\theta^5$. $P(\text{Type I Error})\leq (1/2)^5$. $\theta > (1/2)^{1/5}$, which is approximately 0.87.
\end{frame}

\begin{frame}
\frametitle{p-values}
\textbf{Definition 1:}\\
A \textbf{p-value} $p(\mathbf{X})$ is a test statistic satisfying $0 \leq p(\mathbf{x}) \leq 1$ for every sample point $\mathbf{x}$. A p-value is \textbf{valid} if, for every $\theta \in \Theta_0$ and every $0 \leq \alpha \leq 1$,
$$
P_{\theta}(p(\mathbf{X}) \leq \alpha) \leq \alpha.
$$
\begin{itemize}
\item If we observe $\mathbf{X} = \mathbf{x}$, then for any $\alpha \geq p(\mathbf{x})$, a level $\alpha$ test rejects $H_0$;
\item p-value is essentially a summary statistic of the data. Small values of $p(\mathbf{X})$ give evidence that $H_1$ is true.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{p-values (Cont'd)}
\textbf{Definition 2:}\\
Let $W(\mathbf{X})$ be a test statistic such that large values of $W$ give evidence that $H_1$ is true. For each sample point $\mathbf{x}$, define
$$
p(\mathbf{x}) = \sup_{\theta \in \Theta_0} P_{\theta}(W(\mathbf{X}) \geq W(\mathbf{x})).
$$
Then $p(\mathbf{X})$ is a valid p-value.
\\~\\
\begin{itemize}
\item ``p-value'': the probability of obtaining a sample ``more extreme'' than the ones observed in the data, assuming $H_0$ is true.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{p-values: An Exercise}
A neurologist is testing the effect of a drug on response time by injecting $100$ rats with a unit dose of the drug, subjecting each to neurological stimulus, and recording its response time. The neurologist knows that the response time for a rat not injected with the drug follows a normal distribution with a mean response time of $1.2$ seconds. The mean of the $100$ injected rats' response times is $1.05$ seconds with a sample standard deviation of $0.5$ seconds. 
\\~\\
Do you suggest that the neurologist conclude that the drug has an effect on response time?
\end{frame}

\begin{frame}
\frametitle{Solution to the Exercise}
Suppose the mean response time for rats injected with the drug is $\mu$, then we want to test
$$
H_0: \mu = 1.2s \text{ (the drug has no effect) }
$$
against
$$
H_1: \mu \neq 1.2s \text{ (the drug has effect) }.
$$
Construct the test statistic (here $\bar{X}$ is the sample mean, and $S$ is the sample standard deviation)
$$
Z = \frac{\bar{X} - 1.2}{S/\sqrt{100}}.
$$
$Z \sim t_{99}$, which is approximately $N(0,1)$. Plug in the observed data, $\bar{x} = 1.05, s = 0.5$, and $z=-3$, so the p-value is approximately $P(\vert W \vert \geq \vert z \vert) = P(\vert W \vert \geq 3) \approx 0.003$ (let $W \sim N(0,1)$).%, suggesting strong evidence that $H_1$ is true.
\end{frame}


%------------------------------------------------
\section{Interval Estimation}
\begin{frame}
\frametitle{Interval Estimation}
\begin{itemize}
\item An \textbf{interval estimate} of a parameter $\theta$ is any pair of functions, $L(x_1,\ldots,x_n)$ and $U(x_1,\ldots,x_n)$, of a sample that satisfy $L(\mathbf{x}) \leq U(\mathbf{x})$ for all $\mathbf{x} \in \mathcal{X}$. The inference $L(\mathbf{x}) \leq \theta \leq U(\mathbf{x})$ is made once $\mathbf{X} = \mathbf{x}$ is observed. The \textbf{random interval} $[L(\mathbf{X}),U(\mathbf{X})]$ is called an \textbf{interval estimator}.
\item The \textbf{coverage probability} of an interval estimator $[L(\mathbf{X}),U(\mathbf{X})]$ of a parameter $\theta$ is the probability that the random interval $[L(\mathbf{X}),U(\mathbf{X})]$ covers the true parameter, $\theta$. It is denoted by $P(\theta \in [L(\mathbf{X}),U(\mathbf{X})])$, or $P(\theta \in [L(\mathbf{X}),U(\mathbf{X})]|\theta)$.
\item The \textbf{confidence coefficient} of an interval estimator $[L(\mathbf{X}),U(\mathbf{X})]$ of a parameter $\theta$ is the infimum of the coverage probabilities for all values of $\theta$, $\inf_{\theta}P(\theta \in [L(\mathbf{X}),U(\mathbf{X})])$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Interval Estimation: Key Points}
\begin{enumerate}
\item The \textbf{interval} is the random quantity, not the parameter; 
\item \textbf{``Confidence intervals/sets''}: interval estimators with a measure of confidence (a confidence coefficient); eg. a confidence interval/set with confidence coefficient equal to $C$, is called a ``$C$ confidence interval/set''.
\item The \textbf{coverage probability} is a function of $\theta$, whose true value is unknown, so we can only guarantee the infimum of the coverage probability, the confidence coefficient.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{A mini-exercise}
Suppose that $X$ is a random sample from a distribution with parameter $\theta$, and $[L(X),U(X)]$ is a 95\% confidence interval of $\theta$. If we observe $X=x$, which of the following statements is correct?
\begin{itemize}
\item[A] The probability that $\theta \in [L(x),U(x)]$ is 0.95;
\item[B] The probability that $\theta \in [L(x),U(x)]$ is either 1 or 0.
\end{itemize}
\end{frame}

% \begin{frame}
% \frametitle{Find Interval Estimators Through Pivot Quantities}
% \begin{itemize}
% \item A random variable $Q(\mathbf{X},\mathbf{\theta})$ is a \textbf{pivot quantity} if the distribution of $Q(\mathbf{X},\mathbf{\theta})$ is independent of all parameters.
% \item Usually, $Q(\mathbf{X},\mathbf{\theta})$ contains both parameters and statistics, but for any set $\mathcal{A}$, $P_{\theta}(Q(\mathbf{X},\mathbf{\theta})\in \mathcal{A})$ does not depend on $\theta$.
% \item The goal is to find a pivot quantity $Q(\mathbf{x},\mathbf{\theta})$ and a set $\mathcal{A}$ such that the set $\{\mathbf{\theta}:Q(\mathbf{x},\mathbf{\theta})\in \mathcal{A}\}$ is a set estimate of $\mathbf{\theta}$.
% \end{itemize}
% \end{frame}

\begin{frame}
\frametitle{Example: Normal Confidence Interval}
If $X_1,\ldots,X_n$ are i.i.d. $N(\mu,\sigma^2)$ with $\sigma^2$ known, then $Z=(\bar{X}-\mu)/(\sigma/\sqrt{n})$ is a standard normal variable ($Z \sim N(0,1)$). Then a confidence interval of $\mu$ can be
$$
\{\mu: \bar{x}-a\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{x}+a\frac{\sigma}{\sqrt{n}}\},
$$
where $a$ is a constant. \\
If $\sigma^2$ is unknown, then $T_{n-1}=(\bar{X}-\mu)/(S/\sqrt{n}) \sim t_{n-1}$ which is independent of $\mu$. Thus, for any given $\alpha \in (0,1)$, a $1-\alpha$ confidence interval of $\mu$ is given by
$$
\{\mu: \bar{x}-t_{n-1,(1-\alpha/2)}\frac{s}{\sqrt{n}} \leq \mu \leq \bar{x}+t_{n-1,(1-\alpha/2)}\frac{s}{\sqrt{n}}\},
$$
where $t_{df,p}$ is the $p\times 100\%$th quantile of a student-$t$ distribution with $df$ degrees of freedom.
\end{frame}



% \begin{frame}
% \frametitle{Hypothesis Testing \& Confidence Sets}
% \begin{theorem}
% For each $\theta_0 \in \Theta_0$, let $A(\theta_0)$ be the acceptance region of a level $\alpha$ test of $H_0: \theta=\theta_0$. For each $\mathbf{x}\in \mathcal{X}$, define a set $C(\mathbf{x})$ in the parameter space by
% $$
% C(\mathbf{x})=\{\theta_0:\mathbf{x}\in A(\theta_0)\}.
% $$
% Then the random set $C(\mathbf{x})$ is a $1-\alpha$ confidence set. Conversely, let $C(\mathbf{x})$ be a $C$ confidence set, then for any $\theta_0\in \Theta$,
% $$
% A(\theta_0) = \{\mathbf{x}:\theta_0 \in C(\mathbf{x})\}
% $$
% is the acceptance region of a level $1-C$ test of $H_0: \theta=\theta_0$.
% \end{theorem}
% \end{frame}

% \begin{frame}
% \frametitle{Review Exercises: Afternoon Session}

% \end{frame}

%------------------------------------------------

\section{Introduction to Bayesian Analysis}
\begin{frame}
\frametitle{Video Tutorial}
%\href{https://youtu.be/5NMxiOGL39M}{\textbf{How Bayes Theorem Works}, by \textit{Brandon Rohrer}}
%\\~\\
%OR
%\\~\\
\href{https://youtu.be/3OJEae7Qb_o?t=3m7s}{\textbf{Introduction to Bayesian Data Analysis}, by \textit{asmusab}}
\\~\\
Exercise:\\
Python: \url{https://goo.gl//ceShN5}\\
R: \url{https://goo.gl//cxfnYK}
\end{frame}

\begin{frame}
\frametitle{Bayesian Analysis: the Basics}
\textbf{Two quantities of interest:}
\begin{enumerate}
\item $y \in \mathcal{Y}$: the data ($\mathcal{Y}$: the sample space), a subset of members of the population of interest;
\item $\theta \in \Theta$: the parameter ($\Theta$: the parameter space), expressing the population characteristics.
\end{enumerate}
~\\
\textbf{Three distributions:}
\begin{enumerate}
\item For each numerical value $\theta \in \Theta$, the \textbf{prior distribution} $p(\theta)$ describes our belief that $\theta$ represents the true population characteristics;
\item For each $\theta \in \Theta$ and $y \in \mathcal{Y}$, the \textbf{sampling model} $p(y|\theta)$ describes our belief that $y$ would be the outcome of the study if we knew $\theta$ to be true;
\item For each numerical value of $\theta \in \Theta$, the \textbf{posterior distribution} $p(\theta|y)$ describes our belief that $\theta$ is the true value, having observed dataset $y$.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Posterior Distribution}
The posterior distribution is obtained from the prior distribution and sampling model via \textbf{Bayes' rule}:
$$
p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} = \frac{p(y|\theta)p(\theta)}{\int_{\Theta}p(y|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}.
$$
The Bayes' rule tells us how our beliefs should change after seeing new information.

In practice, however, since evaluating $\int_{\Theta}p(y|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}$ is often intractable, the posterior is instead obtained by
$$
p(\theta|y) \propto p(y|\theta)p(\theta),
$$
and the form of the right hand side can help us determine $p(\theta|y)$.
\end{frame}

\begin{frame}
\frametitle{A Binomial Example}
A survey is carried out to study the support rate $\theta$ ($0 < \theta < 1$) of a policy. 100 people are surveyed, and a binary response $Y_i$ is obtained from each person $i$ ($i=1,2,\ldots,100$), $Y_i \sim \text{Bernoulli}(\theta)$ (that is, $Y = \sum_{i=1}^{100}Y_i \sim \text{Binomial}(100,\theta)$). 
\\~\\
Before the survey, we believe that $\theta \sim \text{Beta}(5,5)$, while the result of the survey is $Y=60$. We'd like to obtain the posterior distribution of $\theta$ given the survey outcome.
\end{frame}

\begin{frame}
\frametitle{A Binomial Example (Cont'd)}
The prior distribution is $\theta \sim \text{Beta}(5,5)$, that is
$$
p(\theta) = \frac{\theta^{5-1}(1-\theta)^{5-1}}{B(5,5)}
\propto \theta^{5-1}(1-\theta)^{5-1}.
$$
The sampling distribution is $Y \sim \text{Binomial}(100,\theta)$, that is, for each $\theta \in (0,1)$ and $y=0,1,\ldots,100$,
$$
P(Y=y|\theta) = 
\begin{pmatrix}
100\\
y
\end{pmatrix}
\theta^{y}(1-\theta)^{100-y}.
$$
Using Bayes' rule, the posterior distribution of $\theta$ given that $Y=60$ is
$$
\begin{aligned}
p(\theta|Y=60) &\propto p(Y=60|\theta)p(\theta)\\
& = \theta^{60}(1-\theta)^{100-60} \theta^{5-1}(1-\theta)^{5-1}\\
&= \theta^{65-1}(1-\theta)^{45-1},
\end{aligned}
$$
which has the form of the p.d.f. of a $\text{Beta}(65,45)$ distribution.\\
Thus, we have $\theta|Y=60 \sim \text{Beta}(65,45)$.
\end{frame}

\begin{frame}
\frametitle{Review Exercise: Bayesian Analysis}
Two tennis players, Serena and Venus, have played against each other 13 times in the past decade, with Serena winning 9 times. Assume that the outcome of a match between them is a binary variable $Y$ ($Y=1$ when Serena wins, $Y=0$ when Venus does) that follows $\text{Bernoulli}(\theta)$ where $0 < \theta < 1$ is an unknown parameter, and we are interested in \textbf{estimating $\theta$, Serena's winning rate against Venus}. We further assume that the outcomes of tennis matches, $Y_1,\ldots,Y_{13}$, are independent.
\begin{enumerate}
\item What is the maximum likelihood estimate of $\theta$?
\item Suppose we ask a tennis expert, John, for prior information. John believes that Serena's winning rate is either $50\%$ or $75\%$, and that these values are equally likely. Given the data, which value of $\theta$ do you think is more likely?
\item Another expert, Martina, suggests that we adopt a $\text{Beta}(9,8)$ prior for $\theta$ %given that the Head-to-Head between Serena and Venus was 9-8 in those matches more than 10 years ago.
upon analyzing match outcomes more than 10 years ago.
What is the posterior distribution of $\theta$ given the match outcomes in the past decade? What is the posterior mean of $\theta$?
\end{enumerate}
\end{frame}

% \begin{frame}
% \Huge{\centerline{The End}}
% \end{frame}

\end{document}
